{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dataset\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part has been implemented but if you have to resize the images again uncomment and store in a separate location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (300,300) #specify the final size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in fn2:\n",
    "# #     set_trace()\n",
    "# #     print(n)\n",
    "#     im = Image.open(f'data/condition/{n}')\n",
    "#     im.thumbnail(sz,Image.ANTIALIAS)\n",
    "#     im.save(f'data/montgomery_china_resized/condition/{n}',format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in fn1:\n",
    "# #     set_trace()\n",
    "# #     print(n)\n",
    "#     im = Image.open(f'data/normal/{n}')\n",
    "#     im.thumbnail(sz,Image.ANTIALIAS)\n",
    "#     im.save(f'data/montgomery_china_resized/normal/{n}',format='png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This method of data loader is used if we have 2 folders train and validation and there are separate folders for each class within them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomSizedCrop(290),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "# #     transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "# #                          std = [ 0.229, 0.224, 0.225 ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transform normalize are required if we are using pre trained networks and we need to apply the same mean and std from the dataset it was trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training\n",
    "traindir = os.path.join('data/montgomery_china_resized/', 'train')\n",
    "train = dataset.ImageFolder(traindir, transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for validation\n",
    "valdir = os.path.join('data/montgomery_china_resized/', 'valid')\n",
    "valid = dataset.ImageFolder(valdir, transform)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN Model (2 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(72*72*32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         set_trace()\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN() #instantiating your CNN class\n",
    "cnn.cuda() #shifting it to gpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #defining your loss\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate) #defining your optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iter [10/42] Loss: 0.6801\n",
      "Epoch [1/10], Iter [20/42] Loss: 0.5891\n",
      "Epoch [1/10], Iter [30/42] Loss: 0.7062\n",
      "Epoch [1/10], Iter [40/42] Loss: 0.9009\n",
      "Epoch [2/10], Iter [10/42] Loss: 0.5291\n",
      "Epoch [2/10], Iter [20/42] Loss: 0.5644\n",
      "Epoch [2/10], Iter [30/42] Loss: 0.6723\n",
      "Epoch [2/10], Iter [40/42] Loss: 0.6372\n",
      "Epoch [3/10], Iter [10/42] Loss: 0.7563\n",
      "Epoch [3/10], Iter [20/42] Loss: 0.8041\n",
      "Epoch [3/10], Iter [30/42] Loss: 0.6315\n",
      "Epoch [3/10], Iter [40/42] Loss: 0.5619\n",
      "Epoch [4/10], Iter [10/42] Loss: 0.6482\n",
      "Epoch [4/10], Iter [20/42] Loss: 0.5227\n",
      "Epoch [4/10], Iter [30/42] Loss: 0.5380\n",
      "Epoch [4/10], Iter [40/42] Loss: 1.0973\n",
      "Epoch [5/10], Iter [10/42] Loss: 0.5827\n",
      "Epoch [5/10], Iter [20/42] Loss: 0.6955\n",
      "Epoch [5/10], Iter [30/42] Loss: 0.7515\n",
      "Epoch [5/10], Iter [40/42] Loss: 0.7205\n",
      "Epoch [6/10], Iter [10/42] Loss: 0.5126\n",
      "Epoch [6/10], Iter [20/42] Loss: 0.6561\n",
      "Epoch [6/10], Iter [30/42] Loss: 0.6456\n",
      "Epoch [6/10], Iter [40/42] Loss: 0.6481\n",
      "Epoch [7/10], Iter [10/42] Loss: 0.6613\n",
      "Epoch [7/10], Iter [20/42] Loss: 0.6469\n",
      "Epoch [7/10], Iter [30/42] Loss: 0.4742\n",
      "Epoch [7/10], Iter [40/42] Loss: 0.5910\n",
      "Epoch [8/10], Iter [10/42] Loss: 0.5918\n",
      "Epoch [8/10], Iter [20/42] Loss: 0.6324\n",
      "Epoch [8/10], Iter [30/42] Loss: 0.4799\n",
      "Epoch [8/10], Iter [40/42] Loss: 0.3721\n",
      "Epoch [9/10], Iter [10/42] Loss: 0.7690\n",
      "Epoch [9/10], Iter [20/42] Loss: 0.5145\n",
      "Epoch [9/10], Iter [30/42] Loss: 0.6831\n",
      "Epoch [9/10], Iter [40/42] Loss: 0.6709\n",
      "Epoch [10/10], Iter [10/42] Loss: 0.7214\n",
      "Epoch [10/10], Iter [20/42] Loss: 0.7233\n",
      "Epoch [10/10], Iter [30/42] Loss: 0.6890\n",
      "Epoch [10/10], Iter [40/42] Loss: 0.6166\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "#         set_trace()\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Predicting on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 67 %\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()    # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in valid_loader:\n",
    "    images = Variable(images).cuda()\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure for using this data loader:\n",
    "* All your images are in one folder that you'll pass as data_folder\n",
    "* All the labels for the images are in a file called labels.csv\n",
    "* directory structure: data_folder and labels.csv are inside root_dir\n",
    "* index: for training/validation purpose...it should be a list of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_from_csv(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, label_file, root_dir, data_folder,transform,index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            label_file (string): Name of the label file which contains id,class.\n",
    "            root_dir (string): Directory which contains the labels file and data folder.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            data_folder(string): name of the folder containing the images\n",
    "            index (list): list of indices to pick from the labels file. It's useful when you have to pick train and validation\n",
    "        \"\"\" \n",
    "        self.root_dir = root_dir\n",
    "        self.index = index\n",
    "        self.labels = pd.read_csv(f'{root_dir}{label_file}').iloc[self.index]\n",
    "        self.classes, self.class_to_idx = self.find_classes()\n",
    "        self.data_folder = data_folder\n",
    "        self.imgs = self.make_dataset()\n",
    "        self.transform = transform\n",
    "            \n",
    "    def find_classes(self):\n",
    "        classes = list(self.labels.iloc[:,1].unique())\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def make_dataset(self): \n",
    "        path = self.labels.iloc[:,0].apply(lambda x: os.path.join(self.root_dir,self.data_folder,x))\n",
    "        target = self.labels.iloc[:,1].apply(lambda x: self.class_to_idx[x])\n",
    "        images = list(zip(path,target))\n",
    "\n",
    "        return images\n",
    "    \n",
    "    def pil_loader(self,path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path,target = self.imgs[idx]\n",
    "        img = self.pil_loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = list(range(1000))\n",
    "val_idxs = list(range(1001,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset_from_csv(data_folder='images_transform/',index=train_idx,label_file='nih_label_normal_condition.csv',transform=transform,root_dir='data/')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = Dataset_from_csv(data_folder='images_transform/',index=val_idxs,label_file='nih_label_normal_condition.csv',transform=transform,root_dir='data/')\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
